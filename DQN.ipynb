{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0c196ce7",
      "metadata": {
        "id": "0c196ce7"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ee7f5cba",
      "metadata": {
        "id": "ee7f5cba"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "194fc0c6",
      "metadata": {
        "id": "194fc0c6"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, gamma=0.99, lr=1e-3, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995, enable_wandb=True):\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma # discount factor: how much future rewards are valued\n",
        "        self.epsilon = epsilon_start # exploration rate: probability of choosing a random action\n",
        "        self.epsilon_min = epsilon_end # minimum exploration rate\n",
        "        self.epsilon_decay = epsilon_decay # rate of decay for exploration probability\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = DQN(state_dim, action_dim).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "        \n",
        "        # Only watch with WandB if enabled (during training)\n",
        "        if enable_wandb:\n",
        "            wandb.watch(self.model, log=\"all\", log_freq=10)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:  # exploration\n",
        "            return random.randrange(self.action_dim)\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "        q_values = self.model(state)\n",
        "        return torch.argmax(q_values).item()  # exploitation\n",
        "\n",
        "    def train_step(self, memory, batch_size):\n",
        "        if len(memory) < batch_size:\n",
        "            return 0.0\n",
        "\n",
        "        states, actions, rewards, next_states, dones = memory.sample(batch_size)\n",
        "\n",
        "        states = torch.FloatTensor(states).to(self.device)\n",
        "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
        "        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)\n",
        "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n",
        "        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n",
        "\n",
        "        # Q(s, a)\n",
        "        q_values = self.model(states).gather(1, actions)\n",
        "\n",
        "        # Target: r + Î³ * max_a' Q(next_state, a')\n",
        "        next_q_values = self.model(next_states).max(1)[0].unsqueeze(1)\n",
        "        target_q = rewards + (1 - dones) * self.gamma * next_q_values\n",
        "\n",
        "        loss = self.loss_fn(q_values, target_q.detach())  # Keep .detach()!\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        # ADD THIS ONE LINE:\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "        \n",
        "        self.optimizer.step()\n",
        "\n",
        "        # decay epsilon\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "            \n",
        "        return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2aa2c5e8",
      "metadata": {
        "id": "2aa2c5e8"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.memory = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return (\n",
        "            np.array(states),\n",
        "            np.array(actions),\n",
        "            np.array(rewards),\n",
        "            np.array(next_states),\n",
        "            np.array(dones)\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4eb58368",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eb58368",
        "outputId": "36fef24d-a46e-4944-c511-742d7f39c202"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dqn_experiment_2</strong> at: <a href='https://wandb.ai/ziadasar-cairo-university/dqn-cartpole/runs/e192ov3g' target=\"_blank\">https://wandb.ai/ziadasar-cairo-university/dqn-cartpole/runs/e192ov3g</a><br> View project at: <a href='https://wandb.ai/ziadasar-cairo-university/dqn-cartpole' target=\"_blank\">https://wandb.ai/ziadasar-cairo-university/dqn-cartpole</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20251112_150923-e192ov3g\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.22.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>d:\\Ziad\\university\\Year 5\\Reinforcement Learning\\Assignment2\\wandb\\run-20251112_151209-z4m4st93</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ziadasar-cairo-university/dqn-cartpole/runs/z4m4st93' target=\"_blank\">dqn_experiment_2</a></strong> to <a href='https://wandb.ai/ziadasar-cairo-university/dqn-cartpole' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/ziadasar-cairo-university/dqn-cartpole' target=\"_blank\">https://wandb.ai/ziadasar-cairo-university/dqn-cartpole</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/ziadasar-cairo-university/dqn-cartpole/runs/z4m4st93' target=\"_blank\">https://wandb.ai/ziadasar-cairo-university/dqn-cartpole/runs/z4m4st93</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "state dimension: 4, action dimension: 2\n",
            "Starting training...\n",
            "Episode 1, Reward: 32.0, Epsilon: 1.000, Avg Loss: 0.0000\n",
            "Episode 2, Reward: 53.0, Epsilon: 0.896, Avg Loss: 1.0925\n",
            "Episode 3, Reward: 35.0, Epsilon: 0.751, Avg Loss: 1.1046\n",
            "Episode 4, Reward: 29.0, Epsilon: 0.650, Avg Loss: 1.0659\n",
            "Episode 5, Reward: 26.0, Epsilon: 0.570, Avg Loss: 1.0489\n",
            "Episode 6, Reward: 14.0, Epsilon: 0.532, Avg Loss: 1.0501\n",
            "Episode 7, Reward: 13.0, Epsilon: 0.498, Avg Loss: 1.0589\n",
            "Episode 8, Reward: 16.0, Epsilon: 0.460, Avg Loss: 1.0748\n",
            "Episode 9, Reward: 13.0, Epsilon: 0.431, Avg Loss: 1.0980\n",
            "Episode 10, Reward: 12.0, Epsilon: 0.406, Avg Loss: 1.1360\n",
            "Episode 11, Reward: 10.0, Epsilon: 0.386, Avg Loss: 1.2318\n",
            "Episode 12, Reward: 14.0, Epsilon: 0.360, Avg Loss: 1.3489\n",
            "Episode 13, Reward: 13.0, Epsilon: 0.337, Avg Loss: 1.4463\n",
            "Episode 14, Reward: 11.0, Epsilon: 0.319, Avg Loss: 1.6021\n",
            "Episode 15, Reward: 11.0, Epsilon: 0.302, Avg Loss: 1.8723\n",
            "Episode 16, Reward: 8.0, Epsilon: 0.290, Avg Loss: 2.1414\n",
            "Episode 17, Reward: 11.0, Epsilon: 0.274, Avg Loss: 2.1796\n",
            "Episode 18, Reward: 9.0, Epsilon: 0.262, Avg Loss: 2.6306\n",
            "Episode 19, Reward: 14.0, Epsilon: 0.245, Avg Loss: 2.7934\n",
            "Episode 20, Reward: 10.0, Epsilon: 0.233, Avg Loss: 2.7440\n",
            "Episode 21, Reward: 12.0, Epsilon: 0.219, Avg Loss: 3.5494\n",
            "Episode 22, Reward: 11.0, Epsilon: 0.207, Avg Loss: 3.7111\n",
            "Episode 23, Reward: 9.0, Epsilon: 0.198, Avg Loss: 3.6961\n",
            "Episode 24, Reward: 9.0, Epsilon: 0.189, Avg Loss: 4.5544\n",
            "Episode 25, Reward: 10.0, Epsilon: 0.180, Avg Loss: 4.9999\n",
            "Episode 26, Reward: 10.0, Epsilon: 0.171, Avg Loss: 4.3816\n",
            "Episode 27, Reward: 10.0, Epsilon: 0.163, Avg Loss: 6.7404\n",
            "Episode 28, Reward: 13.0, Epsilon: 0.153, Avg Loss: 6.2382\n",
            "Episode 29, Reward: 10.0, Epsilon: 0.145, Avg Loss: 6.5314\n",
            "Episode 30, Reward: 11.0, Epsilon: 0.137, Avg Loss: 5.7359\n",
            "Episode 31, Reward: 8.0, Epsilon: 0.132, Avg Loss: 5.3521\n",
            "Episode 32, Reward: 10.0, Epsilon: 0.126, Avg Loss: 6.0144\n",
            "Episode 33, Reward: 10.0, Epsilon: 0.119, Avg Loss: 6.1313\n",
            "Episode 34, Reward: 10.0, Epsilon: 0.114, Avg Loss: 7.4775\n",
            "Episode 35, Reward: 9.0, Epsilon: 0.109, Avg Loss: 7.9224\n",
            "Episode 36, Reward: 11.0, Epsilon: 0.103, Avg Loss: 5.8261\n",
            "Episode 37, Reward: 9.0, Epsilon: 0.098, Avg Loss: 7.1750\n",
            "Episode 38, Reward: 8.0, Epsilon: 0.094, Avg Loss: 7.1209\n",
            "Episode 39, Reward: 9.0, Epsilon: 0.090, Avg Loss: 9.9699\n",
            "Episode 40, Reward: 8.0, Epsilon: 0.087, Avg Loss: 8.7791\n",
            "Episode 41, Reward: 12.0, Epsilon: 0.082, Avg Loss: 8.7537\n",
            "Episode 42, Reward: 8.0, Epsilon: 0.078, Avg Loss: 7.8793\n",
            "Episode 43, Reward: 10.0, Epsilon: 0.075, Avg Loss: 8.7408\n",
            "Episode 44, Reward: 10.0, Epsilon: 0.071, Avg Loss: 9.2618\n",
            "Episode 45, Reward: 11.0, Epsilon: 0.067, Avg Loss: 9.2308\n",
            "Episode 46, Reward: 9.0, Epsilon: 0.064, Avg Loss: 8.9407\n",
            "Episode 47, Reward: 9.0, Epsilon: 0.061, Avg Loss: 5.9791\n",
            "Episode 48, Reward: 11.0, Epsilon: 0.058, Avg Loss: 8.1740\n",
            "Episode 49, Reward: 8.0, Epsilon: 0.056, Avg Loss: 7.0822\n",
            "Episode 50, Reward: 9.0, Epsilon: 0.053, Avg Loss: 6.7173\n",
            "Episode 51, Reward: 8.0, Epsilon: 0.051, Avg Loss: 7.0828\n",
            "Episode 52, Reward: 10.0, Epsilon: 0.050, Avg Loss: 7.1204\n",
            "Episode 53, Reward: 11.0, Epsilon: 0.050, Avg Loss: 6.9584\n",
            "Episode 54, Reward: 9.0, Epsilon: 0.050, Avg Loss: 6.4596\n",
            "Episode 55, Reward: 8.0, Epsilon: 0.050, Avg Loss: 7.4264\n",
            "Episode 56, Reward: 9.0, Epsilon: 0.050, Avg Loss: 6.5947\n",
            "Episode 57, Reward: 9.0, Epsilon: 0.050, Avg Loss: 6.4040\n",
            "Episode 58, Reward: 9.0, Epsilon: 0.050, Avg Loss: 5.5832\n",
            "Episode 59, Reward: 10.0, Epsilon: 0.050, Avg Loss: 4.8173\n",
            "Episode 60, Reward: 12.0, Epsilon: 0.050, Avg Loss: 4.6106\n",
            "Episode 61, Reward: 10.0, Epsilon: 0.050, Avg Loss: 4.5604\n",
            "Episode 62, Reward: 10.0, Epsilon: 0.050, Avg Loss: 4.2574\n",
            "Episode 63, Reward: 12.0, Epsilon: 0.050, Avg Loss: 4.1503\n",
            "Episode 64, Reward: 10.0, Epsilon: 0.050, Avg Loss: 4.7849\n",
            "Episode 65, Reward: 11.0, Epsilon: 0.050, Avg Loss: 3.7948\n",
            "Episode 66, Reward: 8.0, Epsilon: 0.050, Avg Loss: 3.9696\n",
            "Episode 67, Reward: 11.0, Epsilon: 0.050, Avg Loss: 3.5971\n",
            "Episode 68, Reward: 10.0, Epsilon: 0.050, Avg Loss: 4.6688\n",
            "Episode 69, Reward: 9.0, Epsilon: 0.050, Avg Loss: 3.9511\n",
            "Episode 70, Reward: 11.0, Epsilon: 0.050, Avg Loss: 3.9730\n",
            "Episode 71, Reward: 12.0, Epsilon: 0.050, Avg Loss: 3.9628\n",
            "Episode 72, Reward: 8.0, Epsilon: 0.050, Avg Loss: 3.2133\n",
            "Episode 73, Reward: 9.0, Epsilon: 0.050, Avg Loss: 3.7018\n",
            "Episode 74, Reward: 11.0, Epsilon: 0.050, Avg Loss: 3.8231\n",
            "Episode 75, Reward: 12.0, Epsilon: 0.050, Avg Loss: 3.8606\n",
            "Episode 76, Reward: 11.0, Epsilon: 0.050, Avg Loss: 3.4034\n",
            "Episode 77, Reward: 13.0, Epsilon: 0.050, Avg Loss: 3.7642\n",
            "Episode 78, Reward: 10.0, Epsilon: 0.050, Avg Loss: 3.9063\n",
            "Episode 79, Reward: 12.0, Epsilon: 0.050, Avg Loss: 3.8347\n",
            "Episode 80, Reward: 13.0, Epsilon: 0.050, Avg Loss: 3.4069\n",
            "Episode 81, Reward: 11.0, Epsilon: 0.050, Avg Loss: 3.7552\n",
            "Episode 82, Reward: 11.0, Epsilon: 0.050, Avg Loss: 4.8938\n",
            "Episode 83, Reward: 14.0, Epsilon: 0.050, Avg Loss: 3.6611\n",
            "Episode 84, Reward: 12.0, Epsilon: 0.050, Avg Loss: 4.3884\n",
            "Episode 85, Reward: 10.0, Epsilon: 0.050, Avg Loss: 3.6294\n",
            "Episode 86, Reward: 14.0, Epsilon: 0.050, Avg Loss: 4.4895\n",
            "Episode 87, Reward: 17.0, Epsilon: 0.050, Avg Loss: 4.1980\n",
            "Episode 88, Reward: 13.0, Epsilon: 0.050, Avg Loss: 3.4341\n",
            "Episode 89, Reward: 14.0, Epsilon: 0.050, Avg Loss: 3.1688\n",
            "Episode 90, Reward: 16.0, Epsilon: 0.050, Avg Loss: 3.7683\n",
            "Episode 91, Reward: 32.0, Epsilon: 0.050, Avg Loss: 3.7018\n",
            "Episode 92, Reward: 9.0, Epsilon: 0.050, Avg Loss: 7.1235\n",
            "Episode 93, Reward: 10.0, Epsilon: 0.050, Avg Loss: 4.2399\n",
            "Episode 94, Reward: 8.0, Epsilon: 0.050, Avg Loss: 10.6472\n",
            "Episode 95, Reward: 8.0, Epsilon: 0.050, Avg Loss: 11.3307\n",
            "Episode 96, Reward: 9.0, Epsilon: 0.050, Avg Loss: 6.8869\n",
            "Episode 97, Reward: 9.0, Epsilon: 0.050, Avg Loss: 11.3691\n",
            "Episode 98, Reward: 9.0, Epsilon: 0.050, Avg Loss: 13.0465\n",
            "Episode 99, Reward: 10.0, Epsilon: 0.050, Avg Loss: 10.6127\n",
            "Episode 100, Reward: 9.0, Epsilon: 0.050, Avg Loss: 9.4798\n",
            "Episode 101, Reward: 11.0, Epsilon: 0.050, Avg Loss: 17.9392\n",
            "Episode 102, Reward: 11.0, Epsilon: 0.050, Avg Loss: 13.8567\n",
            "Episode 103, Reward: 10.0, Epsilon: 0.050, Avg Loss: 12.9705\n",
            "Episode 104, Reward: 8.0, Epsilon: 0.050, Avg Loss: 14.6187\n",
            "Episode 105, Reward: 8.0, Epsilon: 0.050, Avg Loss: 10.8263\n",
            "Episode 106, Reward: 10.0, Epsilon: 0.050, Avg Loss: 11.8521\n",
            "Episode 107, Reward: 9.0, Epsilon: 0.050, Avg Loss: 11.7138\n",
            "Episode 108, Reward: 10.0, Epsilon: 0.050, Avg Loss: 14.0402\n",
            "Episode 109, Reward: 10.0, Epsilon: 0.050, Avg Loss: 13.3837\n",
            "Episode 110, Reward: 9.0, Epsilon: 0.050, Avg Loss: 15.6679\n",
            "Episode 111, Reward: 9.0, Epsilon: 0.050, Avg Loss: 11.5108\n",
            "Episode 112, Reward: 8.0, Epsilon: 0.050, Avg Loss: 11.1714\n",
            "Episode 113, Reward: 9.0, Epsilon: 0.050, Avg Loss: 17.0573\n",
            "Episode 114, Reward: 9.0, Epsilon: 0.050, Avg Loss: 14.8840\n",
            "Episode 115, Reward: 9.0, Epsilon: 0.050, Avg Loss: 13.9905\n",
            "Episode 116, Reward: 8.0, Epsilon: 0.050, Avg Loss: 15.1164\n",
            "Episode 117, Reward: 9.0, Epsilon: 0.050, Avg Loss: 12.7362\n",
            "Episode 118, Reward: 9.0, Epsilon: 0.050, Avg Loss: 16.9595\n",
            "Episode 119, Reward: 9.0, Epsilon: 0.050, Avg Loss: 7.2311\n",
            "Episode 120, Reward: 10.0, Epsilon: 0.050, Avg Loss: 9.4028\n",
            "Episode 121, Reward: 10.0, Epsilon: 0.050, Avg Loss: 11.6432\n",
            "Episode 122, Reward: 33.0, Epsilon: 0.050, Avg Loss: 9.8756\n",
            "Episode 123, Reward: 19.0, Epsilon: 0.050, Avg Loss: 9.0140\n",
            "Episode 124, Reward: 37.0, Epsilon: 0.050, Avg Loss: 7.9689\n",
            "Episode 125, Reward: 20.0, Epsilon: 0.050, Avg Loss: 5.6020\n",
            "Episode 126, Reward: 17.0, Epsilon: 0.050, Avg Loss: 5.9476\n",
            "Episode 127, Reward: 15.0, Epsilon: 0.050, Avg Loss: 6.7890\n",
            "Episode 128, Reward: 14.0, Epsilon: 0.050, Avg Loss: 5.0446\n",
            "Episode 129, Reward: 17.0, Epsilon: 0.050, Avg Loss: 5.3811\n",
            "Episode 130, Reward: 16.0, Epsilon: 0.050, Avg Loss: 5.4487\n",
            "Episode 131, Reward: 14.0, Epsilon: 0.050, Avg Loss: 3.8485\n",
            "Episode 132, Reward: 21.0, Epsilon: 0.050, Avg Loss: 4.8451\n",
            "Episode 133, Reward: 25.0, Epsilon: 0.050, Avg Loss: 5.4400\n",
            "Episode 134, Reward: 25.0, Epsilon: 0.050, Avg Loss: 4.7437\n",
            "Episode 135, Reward: 24.0, Epsilon: 0.050, Avg Loss: 5.3594\n",
            "Episode 136, Reward: 27.0, Epsilon: 0.050, Avg Loss: 3.8065\n",
            "Episode 137, Reward: 29.0, Epsilon: 0.050, Avg Loss: 4.4787\n",
            "Episode 138, Reward: 21.0, Epsilon: 0.050, Avg Loss: 6.1426\n",
            "Episode 139, Reward: 20.0, Epsilon: 0.050, Avg Loss: 5.1390\n",
            "Episode 140, Reward: 24.0, Epsilon: 0.050, Avg Loss: 5.9928\n",
            "Episode 141, Reward: 23.0, Epsilon: 0.050, Avg Loss: 6.1823\n",
            "Episode 142, Reward: 40.0, Epsilon: 0.050, Avg Loss: 6.1411\n",
            "Episode 143, Reward: 49.0, Epsilon: 0.050, Avg Loss: 7.6024\n",
            "Episode 144, Reward: 52.0, Epsilon: 0.050, Avg Loss: 7.3901\n",
            "Episode 145, Reward: 47.0, Epsilon: 0.050, Avg Loss: 7.5739\n",
            "Episode 146, Reward: 109.0, Epsilon: 0.050, Avg Loss: 8.4170\n",
            "Episode 147, Reward: 87.0, Epsilon: 0.050, Avg Loss: 10.3245\n",
            "Episode 148, Reward: 223.0, Epsilon: 0.050, Avg Loss: 10.6781\n",
            "Episode 149, Reward: 68.0, Epsilon: 0.050, Avg Loss: 13.0597\n",
            "Episode 150, Reward: 87.0, Epsilon: 0.050, Avg Loss: 13.0172\n",
            "Episode 151, Reward: 90.0, Epsilon: 0.050, Avg Loss: 14.2026\n",
            "Episode 152, Reward: 90.0, Epsilon: 0.050, Avg Loss: 17.1574\n",
            "Episode 153, Reward: 47.0, Epsilon: 0.050, Avg Loss: 14.0998\n",
            "Episode 154, Reward: 67.0, Epsilon: 0.050, Avg Loss: 19.8095\n",
            "Episode 155, Reward: 95.0, Epsilon: 0.050, Avg Loss: 21.1240\n",
            "Episode 156, Reward: 129.0, Epsilon: 0.050, Avg Loss: 20.7995\n",
            "Episode 157, Reward: 99.0, Epsilon: 0.050, Avg Loss: 21.6507\n",
            "Episode 158, Reward: 64.0, Epsilon: 0.050, Avg Loss: 25.7982\n",
            "Episode 159, Reward: 46.0, Epsilon: 0.050, Avg Loss: 21.5152\n",
            "Episode 160, Reward: 83.0, Epsilon: 0.050, Avg Loss: 28.8857\n",
            "Episode 161, Reward: 97.0, Epsilon: 0.050, Avg Loss: 23.1175\n",
            "Episode 162, Reward: 50.0, Epsilon: 0.050, Avg Loss: 26.2859\n",
            "Episode 163, Reward: 73.0, Epsilon: 0.050, Avg Loss: 26.5977\n",
            "Episode 164, Reward: 54.0, Epsilon: 0.050, Avg Loss: 34.8159\n",
            "Episode 165, Reward: 51.0, Epsilon: 0.050, Avg Loss: 27.5808\n",
            "Episode 166, Reward: 61.0, Epsilon: 0.050, Avg Loss: 36.2024\n",
            "Episode 167, Reward: 51.0, Epsilon: 0.050, Avg Loss: 40.5305\n",
            "Episode 168, Reward: 43.0, Epsilon: 0.050, Avg Loss: 28.5441\n",
            "Episode 169, Reward: 69.0, Epsilon: 0.050, Avg Loss: 33.9917\n",
            "Episode 170, Reward: 44.0, Epsilon: 0.050, Avg Loss: 42.9256\n",
            "Episode 171, Reward: 98.0, Epsilon: 0.050, Avg Loss: 38.4873\n",
            "Episode 172, Reward: 77.0, Epsilon: 0.050, Avg Loss: 40.8818\n",
            "Episode 173, Reward: 49.0, Epsilon: 0.050, Avg Loss: 37.2443\n",
            "Episode 174, Reward: 49.0, Epsilon: 0.050, Avg Loss: 51.0015\n",
            "Episode 175, Reward: 52.0, Epsilon: 0.050, Avg Loss: 35.3302\n",
            "Episode 176, Reward: 58.0, Epsilon: 0.050, Avg Loss: 49.8575\n",
            "Episode 177, Reward: 62.0, Epsilon: 0.050, Avg Loss: 45.2550\n",
            "Episode 178, Reward: 70.0, Epsilon: 0.050, Avg Loss: 46.7080\n",
            "Episode 179, Reward: 92.0, Epsilon: 0.050, Avg Loss: 51.4091\n",
            "Episode 180, Reward: 48.0, Epsilon: 0.050, Avg Loss: 46.2557\n",
            "Episode 181, Reward: 83.0, Epsilon: 0.050, Avg Loss: 45.4644\n",
            "Episode 182, Reward: 131.0, Epsilon: 0.050, Avg Loss: 49.8724\n",
            "Episode 183, Reward: 79.0, Epsilon: 0.050, Avg Loss: 47.6402\n",
            "Episode 184, Reward: 149.0, Epsilon: 0.050, Avg Loss: 52.0663\n",
            "Episode 185, Reward: 125.0, Epsilon: 0.050, Avg Loss: 52.7878\n",
            "Episode 186, Reward: 60.0, Epsilon: 0.050, Avg Loss: 48.4799\n",
            "Episode 187, Reward: 85.0, Epsilon: 0.050, Avg Loss: 38.4218\n",
            "Episode 188, Reward: 85.0, Epsilon: 0.050, Avg Loss: 42.7114\n",
            "Episode 189, Reward: 70.0, Epsilon: 0.050, Avg Loss: 40.3058\n",
            "Episode 190, Reward: 220.0, Epsilon: 0.050, Avg Loss: 46.2535\n",
            "Episode 191, Reward: 58.0, Epsilon: 0.050, Avg Loss: 40.7282\n",
            "Episode 192, Reward: 98.0, Epsilon: 0.050, Avg Loss: 41.7734\n",
            "Episode 193, Reward: 50.0, Epsilon: 0.050, Avg Loss: 57.1073\n",
            "Episode 194, Reward: 223.0, Epsilon: 0.050, Avg Loss: 46.7748\n",
            "Episode 195, Reward: 101.0, Epsilon: 0.050, Avg Loss: 37.3015\n",
            "Episode 196, Reward: 127.0, Epsilon: 0.050, Avg Loss: 52.3063\n",
            "Episode 197, Reward: 90.0, Epsilon: 0.050, Avg Loss: 50.2006\n",
            "Episode 198, Reward: 447.0, Epsilon: 0.050, Avg Loss: 44.4207\n",
            "Episode 199, Reward: 143.0, Epsilon: 0.050, Avg Loss: 43.9769\n",
            "Episode 200, Reward: 89.0, Epsilon: 0.050, Avg Loss: 39.0431\n",
            "Episode 201, Reward: 202.0, Epsilon: 0.050, Avg Loss: 46.4606\n",
            "Episode 202, Reward: 72.0, Epsilon: 0.050, Avg Loss: 36.2243\n",
            "Episode 203, Reward: 171.0, Epsilon: 0.050, Avg Loss: 35.9743\n",
            "Episode 204, Reward: 57.0, Epsilon: 0.050, Avg Loss: 24.7783\n",
            "Episode 205, Reward: 89.0, Epsilon: 0.050, Avg Loss: 41.2954\n",
            "Episode 206, Reward: 49.0, Epsilon: 0.050, Avg Loss: 36.0333\n",
            "Episode 207, Reward: 43.0, Epsilon: 0.050, Avg Loss: 38.1813\n",
            "Episode 208, Reward: 60.0, Epsilon: 0.050, Avg Loss: 38.8071\n",
            "Episode 209, Reward: 91.0, Epsilon: 0.050, Avg Loss: 38.4910\n",
            "Episode 210, Reward: 123.0, Epsilon: 0.050, Avg Loss: 28.3779\n",
            "Episode 211, Reward: 80.0, Epsilon: 0.050, Avg Loss: 30.2843\n",
            "Episode 212, Reward: 56.0, Epsilon: 0.050, Avg Loss: 25.0050\n",
            "Episode 213, Reward: 136.0, Epsilon: 0.050, Avg Loss: 32.4671\n",
            "Episode 214, Reward: 324.0, Epsilon: 0.050, Avg Loss: 23.4322\n",
            "Episode 215, Reward: 91.0, Epsilon: 0.050, Avg Loss: 17.4268\n",
            "Episode 216, Reward: 82.0, Epsilon: 0.050, Avg Loss: 22.1457\n",
            "Episode 217, Reward: 124.0, Epsilon: 0.050, Avg Loss: 16.1749\n",
            "Episode 218, Reward: 150.0, Epsilon: 0.050, Avg Loss: 15.0261\n",
            "Episode 219, Reward: 94.0, Epsilon: 0.050, Avg Loss: 10.7169\n",
            "Episode 220, Reward: 90.0, Epsilon: 0.050, Avg Loss: 12.6211\n",
            "Episode 221, Reward: 114.0, Epsilon: 0.050, Avg Loss: 7.8587\n",
            "Episode 222, Reward: 131.0, Epsilon: 0.050, Avg Loss: 8.6497\n",
            "Episode 223, Reward: 76.0, Epsilon: 0.050, Avg Loss: 9.0766\n",
            "Episode 224, Reward: 121.0, Epsilon: 0.050, Avg Loss: 6.6624\n",
            "Episode 225, Reward: 221.0, Epsilon: 0.050, Avg Loss: 6.4564\n",
            "Episode 226, Reward: 140.0, Epsilon: 0.050, Avg Loss: 5.9631\n",
            "Episode 227, Reward: 120.0, Epsilon: 0.050, Avg Loss: 6.4334\n",
            "Episode 228, Reward: 187.0, Epsilon: 0.050, Avg Loss: 5.7889\n",
            "Episode 229, Reward: 181.0, Epsilon: 0.050, Avg Loss: 4.5490\n",
            "Episode 230, Reward: 500.0, Epsilon: 0.050, Avg Loss: 6.2730\n",
            "Episode 231, Reward: 500.0, Epsilon: 0.050, Avg Loss: 11.9163\n",
            "Episode 232, Reward: 500.0, Epsilon: 0.050, Avg Loss: 33.5816\n",
            "Episode 233, Reward: 120.0, Epsilon: 0.050, Avg Loss: 74.4432\n",
            "Episode 234, Reward: 222.0, Epsilon: 0.050, Avg Loss: 123.6782\n",
            "Episode 235, Reward: 14.0, Epsilon: 0.050, Avg Loss: 14.2893\n",
            "Episode 236, Reward: 9.0, Epsilon: 0.050, Avg Loss: 11.3704\n",
            "Episode 237, Reward: 10.0, Epsilon: 0.050, Avg Loss: 144.5018\n",
            "Episode 238, Reward: 11.0, Epsilon: 0.050, Avg Loss: 194.8835\n",
            "Episode 239, Reward: 11.0, Epsilon: 0.050, Avg Loss: 570.6794\n",
            "Episode 240, Reward: 13.0, Epsilon: 0.050, Avg Loss: 144.8879\n",
            "Episode 241, Reward: 10.0, Epsilon: 0.050, Avg Loss: 197.0575\n",
            "Episode 242, Reward: 13.0, Epsilon: 0.050, Avg Loss: 34.5387\n",
            "Episode 243, Reward: 10.0, Epsilon: 0.050, Avg Loss: 133.0689\n",
            "Episode 244, Reward: 12.0, Epsilon: 0.050, Avg Loss: 116.5387\n",
            "Episode 245, Reward: 12.0, Epsilon: 0.050, Avg Loss: 557.8091\n",
            "Episode 246, Reward: 10.0, Epsilon: 0.050, Avg Loss: 17.2342\n",
            "Episode 247, Reward: 9.0, Epsilon: 0.050, Avg Loss: 145.6163\n",
            "Episode 248, Reward: 10.0, Epsilon: 0.050, Avg Loss: 695.2029\n",
            "Episode 249, Reward: 11.0, Epsilon: 0.050, Avg Loss: 200.0001\n",
            "Episode 250, Reward: 13.0, Epsilon: 0.050, Avg Loss: 106.2624\n",
            "Episode 251, Reward: 9.0, Epsilon: 0.050, Avg Loss: 277.9605\n",
            "Episode 252, Reward: 10.0, Epsilon: 0.050, Avg Loss: 529.1212\n",
            "Episode 253, Reward: 10.0, Epsilon: 0.050, Avg Loss: 167.2905\n",
            "Episode 254, Reward: 10.0, Epsilon: 0.050, Avg Loss: 633.7483\n",
            "Episode 255, Reward: 12.0, Epsilon: 0.050, Avg Loss: 209.9808\n",
            "Episode 256, Reward: 10.0, Epsilon: 0.050, Avg Loss: 786.4570\n",
            "Episode 257, Reward: 10.0, Epsilon: 0.050, Avg Loss: 90.5054\n",
            "Episode 258, Reward: 10.0, Epsilon: 0.050, Avg Loss: 11.1445\n",
            "Episode 259, Reward: 10.0, Epsilon: 0.050, Avg Loss: 258.0787\n",
            "Episode 260, Reward: 10.0, Epsilon: 0.050, Avg Loss: 64.2562\n",
            "Episode 261, Reward: 11.0, Epsilon: 0.050, Avg Loss: 17.0057\n",
            "Episode 262, Reward: 10.0, Epsilon: 0.050, Avg Loss: 607.7605\n",
            "Episode 263, Reward: 11.0, Epsilon: 0.050, Avg Loss: 485.9545\n",
            "Episode 264, Reward: 9.0, Epsilon: 0.050, Avg Loss: 518.8892\n",
            "Episode 265, Reward: 10.0, Epsilon: 0.050, Avg Loss: 811.7154\n",
            "Episode 266, Reward: 11.0, Epsilon: 0.050, Avg Loss: 672.7240\n",
            "Episode 267, Reward: 10.0, Epsilon: 0.050, Avg Loss: 342.8771\n",
            "Episode 268, Reward: 11.0, Epsilon: 0.050, Avg Loss: 58.0372\n",
            "Episode 269, Reward: 11.0, Epsilon: 0.050, Avg Loss: 551.7339\n",
            "Episode 270, Reward: 11.0, Epsilon: 0.050, Avg Loss: 370.4264\n",
            "Episode 271, Reward: 12.0, Epsilon: 0.050, Avg Loss: 577.7430\n",
            "Episode 272, Reward: 18.0, Epsilon: 0.050, Avg Loss: 303.3310\n",
            "Episode 273, Reward: 48.0, Epsilon: 0.050, Avg Loss: 691.7469\n",
            "Episode 274, Reward: 500.0, Epsilon: 0.050, Avg Loss: 405.5560\n",
            "Episode 275, Reward: 144.0, Epsilon: 0.050, Avg Loss: 365.7755\n",
            "Episode 276, Reward: 82.0, Epsilon: 0.050, Avg Loss: 381.4713\n",
            "Episode 277, Reward: 45.0, Epsilon: 0.050, Avg Loss: 654.8980\n",
            "Episode 278, Reward: 34.0, Epsilon: 0.050, Avg Loss: 701.1948\n",
            "Episode 279, Reward: 27.0, Epsilon: 0.050, Avg Loss: 1374.0310\n",
            "Episode 280, Reward: 24.0, Epsilon: 0.050, Avg Loss: 1179.1108\n",
            "Episode 281, Reward: 24.0, Epsilon: 0.050, Avg Loss: 372.0014\n",
            "Episode 282, Reward: 21.0, Epsilon: 0.050, Avg Loss: 1142.2636\n",
            "Episode 283, Reward: 27.0, Epsilon: 0.050, Avg Loss: 360.1786\n",
            "Episode 284, Reward: 22.0, Epsilon: 0.050, Avg Loss: 173.4235\n",
            "Episode 285, Reward: 24.0, Epsilon: 0.050, Avg Loss: 454.0188\n",
            "Episode 286, Reward: 24.0, Epsilon: 0.050, Avg Loss: 955.0802\n",
            "Episode 287, Reward: 16.0, Epsilon: 0.050, Avg Loss: 2197.8655\n",
            "Episode 288, Reward: 20.0, Epsilon: 0.050, Avg Loss: 1066.3425\n",
            "Episode 289, Reward: 20.0, Epsilon: 0.050, Avg Loss: 326.7091\n",
            "Episode 290, Reward: 23.0, Epsilon: 0.050, Avg Loss: 966.5226\n",
            "Episode 291, Reward: 19.0, Epsilon: 0.050, Avg Loss: 749.0441\n",
            "Episode 292, Reward: 14.0, Epsilon: 0.050, Avg Loss: 630.9668\n",
            "Episode 293, Reward: 21.0, Epsilon: 0.050, Avg Loss: 283.5006\n",
            "Episode 294, Reward: 22.0, Epsilon: 0.050, Avg Loss: 2380.5917\n",
            "Episode 295, Reward: 14.0, Epsilon: 0.050, Avg Loss: 703.7138\n",
            "Episode 296, Reward: 15.0, Epsilon: 0.050, Avg Loss: 2193.9828\n",
            "Episode 297, Reward: 19.0, Epsilon: 0.050, Avg Loss: 1571.5528\n",
            "Episode 298, Reward: 17.0, Epsilon: 0.050, Avg Loss: 975.0172\n",
            "Episode 299, Reward: 14.0, Epsilon: 0.050, Avg Loss: 283.3218\n",
            "Episode 300, Reward: 14.0, Epsilon: 0.050, Avg Loss: 800.7921\n",
            "Episode 301, Reward: 14.0, Epsilon: 0.050, Avg Loss: 1221.1232\n",
            "Episode 302, Reward: 16.0, Epsilon: 0.050, Avg Loss: 1749.8704\n",
            "Episode 303, Reward: 16.0, Epsilon: 0.050, Avg Loss: 459.4881\n",
            "Episode 304, Reward: 13.0, Epsilon: 0.050, Avg Loss: 635.6408\n",
            "Episode 305, Reward: 17.0, Epsilon: 0.050, Avg Loss: 1439.7750\n",
            "Episode 306, Reward: 16.0, Epsilon: 0.050, Avg Loss: 1012.7128\n",
            "Episode 307, Reward: 16.0, Epsilon: 0.050, Avg Loss: 422.4353\n",
            "Episode 308, Reward: 19.0, Epsilon: 0.050, Avg Loss: 444.5745\n",
            "Episode 309, Reward: 18.0, Epsilon: 0.050, Avg Loss: 1688.0283\n",
            "Episode 310, Reward: 11.0, Epsilon: 0.050, Avg Loss: 972.4099\n",
            "Episode 311, Reward: 17.0, Epsilon: 0.050, Avg Loss: 1399.1330\n",
            "Episode 312, Reward: 16.0, Epsilon: 0.050, Avg Loss: 1238.0174\n",
            "Episode 313, Reward: 11.0, Epsilon: 0.050, Avg Loss: 1375.0659\n",
            "Episode 314, Reward: 17.0, Epsilon: 0.050, Avg Loss: 1154.9525\n",
            "Episode 315, Reward: 15.0, Epsilon: 0.050, Avg Loss: 932.2359\n",
            "Episode 316, Reward: 16.0, Epsilon: 0.050, Avg Loss: 1415.2703\n",
            "Episode 317, Reward: 16.0, Epsilon: 0.050, Avg Loss: 541.1596\n",
            "Episode 318, Reward: 14.0, Epsilon: 0.050, Avg Loss: 3301.9623\n",
            "Episode 319, Reward: 16.0, Epsilon: 0.050, Avg Loss: 3532.8821\n",
            "Episode 320, Reward: 14.0, Epsilon: 0.050, Avg Loss: 1582.9027\n",
            "Episode 321, Reward: 18.0, Epsilon: 0.050, Avg Loss: 337.2891\n",
            "Episode 322, Reward: 17.0, Epsilon: 0.050, Avg Loss: 1266.8297\n",
            "Episode 323, Reward: 17.0, Epsilon: 0.050, Avg Loss: 2019.4863\n",
            "Episode 324, Reward: 21.0, Epsilon: 0.050, Avg Loss: 1137.8127\n",
            "Episode 325, Reward: 16.0, Epsilon: 0.050, Avg Loss: 1593.5948\n",
            "Episode 326, Reward: 21.0, Epsilon: 0.050, Avg Loss: 1547.8353\n",
            "Episode 327, Reward: 18.0, Epsilon: 0.050, Avg Loss: 2491.7509\n",
            "Episode 328, Reward: 24.0, Epsilon: 0.050, Avg Loss: 1458.9301\n",
            "Episode 329, Reward: 11.0, Epsilon: 0.050, Avg Loss: 2256.4103\n",
            "Episode 330, Reward: 22.0, Epsilon: 0.050, Avg Loss: 1287.7334\n",
            "Episode 331, Reward: 24.0, Epsilon: 0.050, Avg Loss: 1298.0252\n",
            "Episode 332, Reward: 107.0, Epsilon: 0.050, Avg Loss: 2096.4651\n",
            "Episode 333, Reward: 169.0, Epsilon: 0.050, Avg Loss: 1731.1095\n",
            "Episode 334, Reward: 231.0, Epsilon: 0.050, Avg Loss: 1564.3438\n",
            "Episode 335, Reward: 223.0, Epsilon: 0.050, Avg Loss: 1830.9568\n",
            "Episode 336, Reward: 232.0, Epsilon: 0.050, Avg Loss: 1434.7320\n",
            "Episode 337, Reward: 197.0, Epsilon: 0.050, Avg Loss: 1267.7468\n",
            "Episode 338, Reward: 185.0, Epsilon: 0.050, Avg Loss: 1218.8953\n",
            "Episode 339, Reward: 210.0, Epsilon: 0.050, Avg Loss: 1452.5086\n",
            "Episode 340, Reward: 166.0, Epsilon: 0.050, Avg Loss: 1193.5596\n",
            "Episode 341, Reward: 143.0, Epsilon: 0.050, Avg Loss: 2121.5249\n",
            "Episode 342, Reward: 162.0, Epsilon: 0.050, Avg Loss: 1649.1649\n",
            "Episode 343, Reward: 168.0, Epsilon: 0.050, Avg Loss: 1836.4548\n",
            "Episode 344, Reward: 189.0, Epsilon: 0.050, Avg Loss: 2321.4770\n",
            "Episode 345, Reward: 170.0, Epsilon: 0.050, Avg Loss: 2162.4890\n",
            "Episode 346, Reward: 135.0, Epsilon: 0.050, Avg Loss: 1593.5114\n",
            "Episode 347, Reward: 162.0, Epsilon: 0.050, Avg Loss: 2218.0916\n",
            "Episode 348, Reward: 162.0, Epsilon: 0.050, Avg Loss: 2183.4233\n",
            "Episode 349, Reward: 197.0, Epsilon: 0.050, Avg Loss: 2271.0554\n",
            "Episode 350, Reward: 169.0, Epsilon: 0.050, Avg Loss: 2800.9356\n",
            "Episode 351, Reward: 178.0, Epsilon: 0.050, Avg Loss: 3308.0445\n",
            "Episode 352, Reward: 143.0, Epsilon: 0.050, Avg Loss: 3254.9733\n",
            "Episode 353, Reward: 139.0, Epsilon: 0.050, Avg Loss: 3233.5729\n",
            "Episode 354, Reward: 140.0, Epsilon: 0.050, Avg Loss: 3336.5205\n",
            "Episode 355, Reward: 167.0, Epsilon: 0.050, Avg Loss: 2685.9232\n",
            "Episode 356, Reward: 141.0, Epsilon: 0.050, Avg Loss: 2792.0238\n",
            "Episode 357, Reward: 158.0, Epsilon: 0.050, Avg Loss: 3392.9883\n",
            "Episode 358, Reward: 154.0, Epsilon: 0.050, Avg Loss: 2545.5720\n",
            "Episode 359, Reward: 151.0, Epsilon: 0.050, Avg Loss: 3532.9825\n",
            "Episode 360, Reward: 145.0, Epsilon: 0.050, Avg Loss: 2814.0497\n",
            "Episode 361, Reward: 148.0, Epsilon: 0.050, Avg Loss: 2878.0062\n",
            "Episode 362, Reward: 154.0, Epsilon: 0.050, Avg Loss: 3749.1595\n",
            "Episode 363, Reward: 141.0, Epsilon: 0.050, Avg Loss: 2907.1413\n",
            "Episode 364, Reward: 150.0, Epsilon: 0.050, Avg Loss: 3768.9776\n",
            "Episode 365, Reward: 146.0, Epsilon: 0.050, Avg Loss: 3001.7497\n",
            "Episode 366, Reward: 148.0, Epsilon: 0.050, Avg Loss: 3228.7182\n",
            "Episode 367, Reward: 138.0, Epsilon: 0.050, Avg Loss: 3219.6907\n",
            "Episode 368, Reward: 129.0, Epsilon: 0.050, Avg Loss: 3560.5739\n",
            "Episode 369, Reward: 138.0, Epsilon: 0.050, Avg Loss: 3145.1012\n",
            "Episode 370, Reward: 134.0, Epsilon: 0.050, Avg Loss: 2825.5699\n",
            "Episode 371, Reward: 149.0, Epsilon: 0.050, Avg Loss: 2880.4933\n",
            "Episode 372, Reward: 129.0, Epsilon: 0.050, Avg Loss: 3113.2170\n",
            "Episode 373, Reward: 133.0, Epsilon: 0.050, Avg Loss: 3428.9314\n",
            "Episode 374, Reward: 132.0, Epsilon: 0.050, Avg Loss: 3261.4974\n",
            "Episode 375, Reward: 137.0, Epsilon: 0.050, Avg Loss: 2066.2507\n",
            "Episode 376, Reward: 130.0, Epsilon: 0.050, Avg Loss: 2101.7126\n",
            "Episode 377, Reward: 151.0, Epsilon: 0.050, Avg Loss: 2594.9995\n",
            "Episode 378, Reward: 143.0, Epsilon: 0.050, Avg Loss: 2147.3890\n",
            "Episode 379, Reward: 139.0, Epsilon: 0.050, Avg Loss: 1765.7388\n",
            "Episode 380, Reward: 158.0, Epsilon: 0.050, Avg Loss: 1470.9560\n",
            "Episode 381, Reward: 148.0, Epsilon: 0.050, Avg Loss: 862.0112\n",
            "Episode 382, Reward: 166.0, Epsilon: 0.050, Avg Loss: 644.9204\n",
            "Episode 383, Reward: 153.0, Epsilon: 0.050, Avg Loss: 574.9144\n",
            "Episode 384, Reward: 173.0, Epsilon: 0.050, Avg Loss: 234.3280\n",
            "Episode 385, Reward: 178.0, Epsilon: 0.050, Avg Loss: 49.5867\n",
            "Episode 386, Reward: 212.0, Epsilon: 0.050, Avg Loss: 28.3077\n",
            "Episode 387, Reward: 265.0, Epsilon: 0.050, Avg Loss: 23.9195\n",
            "Episode 388, Reward: 482.0, Epsilon: 0.050, Avg Loss: 22.5112\n",
            "Episode 389, Reward: 258.0, Epsilon: 0.050, Avg Loss: 21.3389\n",
            "Episode 390, Reward: 320.0, Epsilon: 0.050, Avg Loss: 33.4432\n",
            "Episode 391, Reward: 500.0, Epsilon: 0.050, Avg Loss: 65.4143\n",
            "Episode 392, Reward: 500.0, Epsilon: 0.050, Avg Loss: 441.6113\n",
            "Episode 393, Reward: 332.0, Epsilon: 0.050, Avg Loss: 1265.2213\n",
            "Episode 394, Reward: 262.0, Epsilon: 0.050, Avg Loss: 2061.9583\n",
            "Episode 395, Reward: 187.0, Epsilon: 0.050, Avg Loss: 3120.4938\n",
            "Episode 396, Reward: 187.0, Epsilon: 0.050, Avg Loss: 4660.1712\n",
            "Episode 397, Reward: 169.0, Epsilon: 0.050, Avg Loss: 5247.8223\n",
            "Episode 398, Reward: 159.0, Epsilon: 0.050, Avg Loss: 6482.5288\n",
            "Episode 399, Reward: 159.0, Epsilon: 0.050, Avg Loss: 6062.1307\n",
            "Episode 400, Reward: 141.0, Epsilon: 0.050, Avg Loss: 6927.4304\n",
            "Episode 401, Reward: 128.0, Epsilon: 0.050, Avg Loss: 11077.0068\n",
            "Episode 402, Reward: 127.0, Epsilon: 0.050, Avg Loss: 9426.0664\n",
            "Episode 403, Reward: 123.0, Epsilon: 0.050, Avg Loss: 9447.3343\n",
            "Episode 404, Reward: 13.0, Epsilon: 0.050, Avg Loss: 6413.5148\n",
            "Episode 405, Reward: 10.0, Epsilon: 0.050, Avg Loss: 8126.4006\n",
            "Episode 406, Reward: 10.0, Epsilon: 0.050, Avg Loss: 9121.7521\n",
            "Episode 407, Reward: 10.0, Epsilon: 0.050, Avg Loss: 10156.8236\n",
            "Episode 408, Reward: 10.0, Epsilon: 0.050, Avg Loss: 759.5303\n",
            "Episode 409, Reward: 11.0, Epsilon: 0.050, Avg Loss: 8487.4768\n",
            "Episode 410, Reward: 9.0, Epsilon: 0.050, Avg Loss: 15750.4791\n",
            "Episode 411, Reward: 10.0, Epsilon: 0.050, Avg Loss: 14807.3523\n",
            "Episode 412, Reward: 12.0, Epsilon: 0.050, Avg Loss: 9373.2959\n",
            "Episode 413, Reward: 10.0, Epsilon: 0.050, Avg Loss: 10093.4433\n",
            "Episode 414, Reward: 10.0, Epsilon: 0.050, Avg Loss: 10702.7361\n",
            "Episode 415, Reward: 9.0, Epsilon: 0.050, Avg Loss: 21722.9290\n",
            "Episode 416, Reward: 10.0, Epsilon: 0.050, Avg Loss: 25408.4542\n",
            "Episode 417, Reward: 10.0, Epsilon: 0.050, Avg Loss: 5814.4036\n",
            "Episode 418, Reward: 10.0, Epsilon: 0.050, Avg Loss: 35299.2793\n",
            "Episode 419, Reward: 11.0, Epsilon: 0.050, Avg Loss: 112.7128\n",
            "Episode 420, Reward: 14.0, Epsilon: 0.050, Avg Loss: 23634.9503\n",
            "Episode 421, Reward: 13.0, Epsilon: 0.050, Avg Loss: 12739.5018\n",
            "Episode 422, Reward: 139.0, Epsilon: 0.050, Avg Loss: 12392.9836\n",
            "Episode 423, Reward: 169.0, Epsilon: 0.050, Avg Loss: 20793.5102\n",
            "Episode 424, Reward: 319.0, Epsilon: 0.050, Avg Loss: 17594.9401\n",
            "Episode 425, Reward: 122.0, Epsilon: 0.050, Avg Loss: 9380.0273\n",
            "Episode 426, Reward: 103.0, Epsilon: 0.050, Avg Loss: 11031.8060\n",
            "Episode 427, Reward: 102.0, Epsilon: 0.050, Avg Loss: 12993.1134\n",
            "Episode 428, Reward: 98.0, Epsilon: 0.050, Avg Loss: 10712.4846\n",
            "Episode 429, Reward: 111.0, Epsilon: 0.050, Avg Loss: 13213.7114\n",
            "Episode 430, Reward: 98.0, Epsilon: 0.050, Avg Loss: 18151.0517\n",
            "Episode 431, Reward: 95.0, Epsilon: 0.050, Avg Loss: 17729.3901\n",
            "Episode 432, Reward: 102.0, Epsilon: 0.050, Avg Loss: 15872.6475\n",
            "Episode 433, Reward: 74.0, Epsilon: 0.050, Avg Loss: 15966.2456\n",
            "Episode 434, Reward: 53.0, Epsilon: 0.050, Avg Loss: 14629.9368\n",
            "Episode 435, Reward: 51.0, Epsilon: 0.050, Avg Loss: 18809.0010\n",
            "Episode 436, Reward: 42.0, Epsilon: 0.050, Avg Loss: 25284.5200\n",
            "Episode 437, Reward: 36.0, Epsilon: 0.050, Avg Loss: 16770.7404\n",
            "Episode 438, Reward: 40.0, Epsilon: 0.050, Avg Loss: 20354.8053\n",
            "Episode 439, Reward: 41.0, Epsilon: 0.050, Avg Loss: 14966.3339\n",
            "Episode 440, Reward: 27.0, Epsilon: 0.050, Avg Loss: 20428.9652\n",
            "Episode 441, Reward: 34.0, Epsilon: 0.050, Avg Loss: 17922.7783\n",
            "Episode 442, Reward: 19.0, Epsilon: 0.050, Avg Loss: 11480.0577\n",
            "Episode 443, Reward: 27.0, Epsilon: 0.050, Avg Loss: 14124.3547\n",
            "Episode 444, Reward: 32.0, Epsilon: 0.050, Avg Loss: 21478.0153\n",
            "Episode 445, Reward: 29.0, Epsilon: 0.050, Avg Loss: 18455.8944\n",
            "Episode 446, Reward: 26.0, Epsilon: 0.050, Avg Loss: 18586.7294\n",
            "Episode 447, Reward: 23.0, Epsilon: 0.050, Avg Loss: 8858.2423\n",
            "Episode 448, Reward: 33.0, Epsilon: 0.050, Avg Loss: 26116.9330\n",
            "Episode 449, Reward: 41.0, Epsilon: 0.050, Avg Loss: 15760.0484\n",
            "Episode 450, Reward: 47.0, Epsilon: 0.050, Avg Loss: 27080.0262\n",
            "Episode 451, Reward: 47.0, Epsilon: 0.050, Avg Loss: 13186.9415\n",
            "Episode 452, Reward: 95.0, Epsilon: 0.050, Avg Loss: 22899.1800\n",
            "Episode 453, Reward: 110.0, Epsilon: 0.050, Avg Loss: 19194.0004\n",
            "Episode 454, Reward: 114.0, Epsilon: 0.050, Avg Loss: 21358.0503\n",
            "Episode 455, Reward: 115.0, Epsilon: 0.050, Avg Loss: 13062.1510\n",
            "Episode 456, Reward: 130.0, Epsilon: 0.050, Avg Loss: 14360.0770\n",
            "Episode 457, Reward: 122.0, Epsilon: 0.050, Avg Loss: 10370.7868\n",
            "Episode 458, Reward: 132.0, Epsilon: 0.050, Avg Loss: 9284.9169\n",
            "Episode 459, Reward: 95.0, Epsilon: 0.050, Avg Loss: 8301.8840\n",
            "Episode 460, Reward: 104.0, Epsilon: 0.050, Avg Loss: 6770.7067\n",
            "Episode 461, Reward: 111.0, Epsilon: 0.050, Avg Loss: 5468.4726\n",
            "Episode 462, Reward: 128.0, Epsilon: 0.050, Avg Loss: 4425.1721\n",
            "Episode 463, Reward: 132.0, Epsilon: 0.050, Avg Loss: 2642.6180\n",
            "Episode 464, Reward: 100.0, Epsilon: 0.050, Avg Loss: 2093.5239\n",
            "Episode 465, Reward: 118.0, Epsilon: 0.050, Avg Loss: 1304.8927\n",
            "Episode 466, Reward: 138.0, Epsilon: 0.050, Avg Loss: 968.1701\n",
            "Episode 467, Reward: 114.0, Epsilon: 0.050, Avg Loss: 577.7968\n",
            "Episode 468, Reward: 127.0, Epsilon: 0.050, Avg Loss: 343.4245\n",
            "Episode 469, Reward: 142.0, Epsilon: 0.050, Avg Loss: 182.9994\n",
            "Episode 470, Reward: 217.0, Epsilon: 0.050, Avg Loss: 183.9708\n",
            "Episode 471, Reward: 407.0, Epsilon: 0.050, Avg Loss: 489.6402\n",
            "Episode 472, Reward: 500.0, Epsilon: 0.050, Avg Loss: 856.7999\n",
            "Episode 473, Reward: 241.0, Epsilon: 0.050, Avg Loss: 852.6916\n",
            "Episode 474, Reward: 190.0, Epsilon: 0.050, Avg Loss: 1174.7279\n",
            "Episode 475, Reward: 173.0, Epsilon: 0.050, Avg Loss: 1052.0407\n",
            "Episode 476, Reward: 190.0, Epsilon: 0.050, Avg Loss: 1416.7010\n",
            "Episode 477, Reward: 179.0, Epsilon: 0.050, Avg Loss: 1358.5988\n",
            "Episode 478, Reward: 157.0, Epsilon: 0.050, Avg Loss: 926.8769\n",
            "Episode 479, Reward: 156.0, Epsilon: 0.050, Avg Loss: 504.4287\n",
            "Episode 480, Reward: 168.0, Epsilon: 0.050, Avg Loss: 520.3333\n",
            "Episode 481, Reward: 169.0, Epsilon: 0.050, Avg Loss: 389.7450\n",
            "Episode 482, Reward: 190.0, Epsilon: 0.050, Avg Loss: 194.6442\n",
            "Episode 483, Reward: 201.0, Epsilon: 0.050, Avg Loss: 118.4514\n",
            "Episode 484, Reward: 215.0, Epsilon: 0.050, Avg Loss: 60.7797\n",
            "Episode 485, Reward: 282.0, Epsilon: 0.050, Avg Loss: 82.9799\n",
            "Episode 486, Reward: 500.0, Epsilon: 0.050, Avg Loss: 253.0612\n",
            "Episode 487, Reward: 495.0, Epsilon: 0.050, Avg Loss: 979.9398\n",
            "Episode 488, Reward: 500.0, Epsilon: 0.050, Avg Loss: 2289.9660\n",
            "Episode 489, Reward: 370.0, Epsilon: 0.050, Avg Loss: 5147.1938\n",
            "Episode 490, Reward: 311.0, Epsilon: 0.050, Avg Loss: 7085.7704\n",
            "Episode 491, Reward: 330.0, Epsilon: 0.050, Avg Loss: 8044.3037\n",
            "Episode 492, Reward: 491.0, Epsilon: 0.050, Avg Loss: 14210.9135\n",
            "Episode 493, Reward: 373.0, Epsilon: 0.050, Avg Loss: 17901.5551\n",
            "Episode 494, Reward: 261.0, Epsilon: 0.050, Avg Loss: 16889.0769\n",
            "Episode 495, Reward: 204.0, Epsilon: 0.050, Avg Loss: 21339.5264\n",
            "Episode 496, Reward: 201.0, Epsilon: 0.050, Avg Loss: 15550.2739\n",
            "Episode 497, Reward: 191.0, Epsilon: 0.050, Avg Loss: 18579.5881\n",
            "Episode 498, Reward: 181.0, Epsilon: 0.050, Avg Loss: 20140.6453\n",
            "Episode 499, Reward: 170.0, Epsilon: 0.050, Avg Loss: 28245.0435\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 500, Reward: 142.0, Epsilon: 0.050, Avg Loss: 23411.2636\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>buffer_size</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>episode</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>episode_length</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>epsilon</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>total_reward</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>23411.26361</td></tr><tr><td>buffer_size</td><td>10000</td></tr><tr><td>episode</td><td>500</td></tr><tr><td>episode_length</td><td>142</td></tr><tr><td>epsilon</td><td>0.04991</td></tr><tr><td>total_reward</td><td>142</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dqn_experiment_2</strong> at: <a href='https://wandb.ai/ziadasar-cairo-university/dqn-cartpole/runs/z4m4st93' target=\"_blank\">https://wandb.ai/ziadasar-cairo-university/dqn-cartpole/runs/z4m4st93</a><br> View project at: <a href='https://wandb.ai/ziadasar-cairo-university/dqn-cartpole' target=\"_blank\">https://wandb.ai/ziadasar-cairo-university/dqn-cartpole</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20251112_151209-z4m4st93\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def train_dqn():\n",
        "    wandb.init(project=\"dqn-cartpole\", name=\"dqn_experiment_2\")\n",
        "    \n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    print(f\"state dimension: {state_dim}, action dimension: {action_dim}\")\n",
        "\n",
        "    agent = DQNAgent(state_dim, action_dim, lr=1e-4, epsilon_decay=0.995, epsilon_end=0.05)\n",
        "    memory = ReplayBuffer(10000)\n",
        "    episodes = 500\n",
        "    batch_size = 64\n",
        "\n",
        "    # MOVE CONFIG UPDATE HERE - before training starts\n",
        "    wandb.config.update({\n",
        "        \"state_dim\": state_dim,\n",
        "        \"action_dim\": action_dim,\n",
        "        \"gamma\": agent.gamma,\n",
        "        \"learning_rate\": 1e-4,  # Fixed to match your actual LR\n",
        "        \"epsilon_start\": 1.0,\n",
        "        \"epsilon_end\": 0.05,    # Fixed to match your actual value\n",
        "        \"epsilon_decay\": 0.995, # Fixed to match your actual value\n",
        "        \"batch_size\": batch_size,\n",
        "        \"buffer_size\": 10000,\n",
        "        \"episodes\": episodes\n",
        "    })\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        total_reward = 0\n",
        "        episode_loss = 0\n",
        "        train_steps = 0\n",
        "\n",
        "        for t in range(500):\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            memory.push(state, action, reward, next_state, done)\n",
        "            loss = agent.train_step(memory, batch_size)\n",
        "            \n",
        "            if loss > 0:\n",
        "                episode_loss += loss\n",
        "                train_steps += 1\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Calculate average loss\n",
        "        avg_loss = episode_loss / train_steps if train_steps > 0 else 0\n",
        "        \n",
        "        # FIXED INDENTATION: Log ONCE PER EPISODE\n",
        "        wandb.log({\n",
        "            \"episode\": ep + 1,\n",
        "            \"total_reward\": total_reward,\n",
        "            \"epsilon\": agent.epsilon,\n",
        "            \"avg_loss\": avg_loss,\n",
        "            \"buffer_size\": len(memory),\n",
        "            \"episode_length\": t + 1\n",
        "        })\n",
        "\n",
        "        print(f\"Episode {ep+1}, Reward: {total_reward}, Epsilon: {agent.epsilon:.3f}, Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Save model AFTER training completes\n",
        "    torch.save(agent.model.state_dict(), \"dqn_model.pth\")\n",
        "    wandb.save(\"dqn_model.pth\")\n",
        "    wandb.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_dqn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "60de1e03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60de1e03",
        "outputId": "e4c2ff4f-c6f4-446b-fc40-71796b9cdb53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated True truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated True truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated False truncated False\n",
            "terminated True truncated False\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Create a unique folder for videos\n",
        "video_folder = f\"videos/run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "env = gym.wrappers.TimeLimit(env, max_episode_steps=1000)\n",
        "env = gym.wrappers.RecordVideo(env, video_folder, episode_trigger=lambda e: True)\n",
        "\n",
        "# Load the trained model WITH WandB DISABLED\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "agent = DQNAgent(state_dim, action_dim, enable_wandb=False)  # â ADD THIS PARAMETER\n",
        "agent.model.load_state_dict(torch.load(\"dqn_model.pth\"))\n",
        "agent.epsilon = 0  # Disable exploration\n",
        "\n",
        "# Record multiple episodes\n",
        "for episode in range(3):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.select_action(state)\n",
        "        state, _, terminated, truncated, _ = env.step(action)\n",
        "        print(\"terminated\", terminated, \"truncated\", truncated)\n",
        "        done = terminated or truncated\n",
        "\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
