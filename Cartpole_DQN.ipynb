{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921fd6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import  gymnasium as gym\n",
    "import torch\n",
    "import dqn \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_cartpole():\n",
    "\n",
    "    wandb.init(project=\"dqn-cartpole\", name=\"dqn_experiment_1\")\n",
    "    \n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    # Corrected print statement\n",
    "    print(f\"state dimension: {state_dim}, action dimension: {action_dim}\")\n",
    "\n",
    "    agent = DQNAgent(state_dim, action_dim, lr=5e-4, epsilon_decay=0.99)\n",
    "    memory = ReplayBuffer(10000)\n",
    "    episodes = 500\n",
    "    batch_size = 64\n",
    "\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        episode_loss = 0\n",
    "        train_steps = 0\n",
    "\n",
    "        for t in range(500):\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            memory.push(state, action, reward, next_state, done)\n",
    "            loss = agent.train_step(memory, batch_size)  # Now returns loss\n",
    "            \n",
    "            if loss > 0:  # Only count when training happened\n",
    "                episode_loss += loss\n",
    "                train_steps += 1\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Calculate average loss\n",
    "        avg_loss = episode_loss / train_steps if train_steps > 0 else 0\n",
    "        \n",
    "        # ADD THESE LINES for WandB logging:\n",
    "        wandb.log({\n",
    "            \"episode\": ep + 1,\n",
    "            \"total_reward\": total_reward,\n",
    "            \"epsilon\": agent.epsilon,\n",
    "            \"avg_loss\": avg_loss,\n",
    "            \"buffer_size\": len(memory)\n",
    "        })\n",
    "\n",
    "        print(f\"Episode {ep+1}, Reward: {total_reward}, Epsilon: {agent.epsilon:.3f}, Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # ADD THESE LINES before torch.save:\n",
    "    wandb.config.update({\n",
    "        \"state_dim\": state_dim,\n",
    "        \"action_dim\": action_dim,\n",
    "        \"gamma\": agent.gamma,\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"epsilon_start\": 1.0,\n",
    "        \"epsilon_end\": 0.01,\n",
    "        \"epsilon_decay\": 0.99,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"buffer_size\": 10000,\n",
    "        \"episodes\": episodes\n",
    "    })\n",
    "\n",
    "    torch.save(agent.model.state_dict(), \"dqn_model.pth\")\n",
    "    wandb.save(\"dqn_model.pth\")  # <-- ADD THIS to save model to WandB\n",
    "    wandb.finish()  # <-- ADD THIS at the very end\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_dqn()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
